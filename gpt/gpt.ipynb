{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIlu+BkKgbkS5btViSE2AT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harsh-2909/GenAI-Learning/blob/main/gpt/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16FnjeIxTCnl",
        "outputId": "d697faf6-d939-4e9c-dc72-d08b67ef5bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-15 11:38:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  4.80MB/s    in 0.2s    \n",
            "\n",
            "2024-12-15 11:38:38 (4.80 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Lets get the data first\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "GPwvEK5bTVfh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)\n",
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "tuhgmTK5Tohe",
        "outputId": "2231d8e9-850f-475d-9378-a05c4e387eab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the vocabulary of the training data\n",
        "char = sorted(list(set(text)))\n",
        "print(''.join(char))\n",
        "print(len(char))\n",
        "vocab_size = len(char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YNCjxcJTv21",
        "outputId": "1a26ff2f-fa9c-491f-f94c-8d15072d9ea5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we create a tokenizer to tokenize the inputs\n",
        "# A tokenizer simply converts the inputs into a list of integers where each integer represent a token (which can be a word, sub-word, or character)\n",
        "# There are multiple types of tokenizers like word, sub-word, and character based tokenizer.\n",
        "# Here, we will be using a character tokenizer.\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(char)}\n",
        "itos = {i: ch for i, ch in enumerate(char)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "s = \"\"\"Hey Jude\n",
        "Don't make it bad\"\"\"\n",
        "print(encode(s))\n",
        "print(decode(encode(s)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dk4gUZ1Uz8f",
        "outputId": "c78eda5e-773f-4a51-edd4-91461a86a3f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 63, 1, 22, 59, 42, 43, 0, 16, 53, 52, 5, 58, 1, 51, 39, 49, 43, 1, 47, 58, 1, 40, 39, 42]\n",
            "Hey Jude\n",
            "Don't make it bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we encode the entire text and convert it into tensors using PyTorch\n",
        "import torch\n",
        "encoded_text = encode(text)\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlPzmMTya8Co",
        "outputId": "27033868-e1d4-41de-dd76-deb43bc848d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
              "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
              "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
              "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
              "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
              "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
              "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
              "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
              "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
              "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
              "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
              "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
              "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
              "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
              "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
              "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
              "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
              "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
              "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
              "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
              "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
              "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
              "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
              "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
              "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
              "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
              "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
              "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
              "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
              "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
              "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
              "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
              "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
              "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
              "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
              "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
              "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
              "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
              "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
              "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
              "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
              "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
              "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
              "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
              "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
              "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
              "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
              "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
              "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and validation chunks\n",
        "train_data_percent = 0.9 # 90% would be for training\n",
        "n = int(train_data_percent * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "deVu6bqacGEF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot train a model with all the training data at once, as it is very expensive computationally. Thus, we train the model chunk by chunk where each chunk is taken out randomly from the data. Here, the chunk size will be indicated by the variable `block_size`.\n"
      ],
      "metadata": {
        "id": "gdvn9qxc0kor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "# train_data[:block_size+1]\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtO1fiu41lf-",
        "outputId": "619d8fc6-4491-498a-f859-8b7b8901d63b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The training chunk of block size gives example to the model on how to respond when facing a set of inputs. For example, here when [18] is sent as input then return 47.\n",
        "  Similarly, when [18, 47] is sent as input, then return 56. Thus, this gives us `block_size` number of examples from context length of 1 to block_size.\n",
        "\n",
        "  Here, we will also need to add batch_size to do parallel processing to make training more efficient. Each block in a batch is processed parallelly without any interference."
      ],
      "metadata": {
        "id": "hdllAkab5tH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) # Manually seeding to get deterministic random numbers\n",
        "batch_size = 4 # The number of parallel independent sequences\n",
        "block_size = 8 # The number of tokens processed at a time. Maximum context length of predictions\n",
        "\n",
        "def get_batch(split_type):\n",
        "  \"\"\"Generates a small batch of data of inputs x and target y\"\"\"\n",
        "  data = train_data if split_type == \"train\" else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size, )) # generates a list of offsets randomly of size `batch_size`\n",
        "  x = torch.stack([data[i: i+block_size] for i in ix]) # Here, we are creating the context blocks\n",
        "  y = torch.stack([data[i+1: i+block_size+1] for i in ix]) # Target Blocks. This should be the output when a context block is passed to the model\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"inputs:\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"targets:\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target is {target}\")\n",
        "    # Here, this gives us 32 different context (x) with their required targets (y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-u6fyur6ToA",
        "outputId": "b5dea14e-478c-4a6b-ecb2-c4e7e2af0382"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target is 43\n",
            "when input is [24, 43] the target is 58\n",
            "when input is [24, 43, 58] the target is 5\n",
            "when input is [24, 43, 58, 5] the target is 57\n",
            "when input is [24, 43, 58, 5, 57] the target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
            "when input is [44] the target is 53\n",
            "when input is [44, 53] the target is 56\n",
            "when input is [44, 53, 56] the target is 1\n",
            "when input is [44, 53, 56, 1] the target is 58\n",
            "when input is [44, 53, 56, 1, 58] the target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52] the target is 58\n",
            "when input is [52, 58] the target is 1\n",
            "when input is [52, 58, 1] the target is 58\n",
            "when input is [52, 58, 1, 58] the target is 46\n",
            "when input is [52, 58, 1, 58, 46] the target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
            "when input is [25] the target is 17\n",
            "when input is [25, 17] the target is 27\n",
            "when input is [25, 17, 27] the target is 10\n",
            "when input is [25, 17, 27, 10] the target is 0\n",
            "when input is [25, 17, 27, 10, 0] the target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  \"\"\"We are using the Bigram language model. It is a part of n-gram models. A bigram model uses the context of the previous 1 token to predict the next token. It does not check the context beyond 1 token.\"\"\"\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits from the lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    logits = self.token_embedding_table(idx) # Returns a tensor of dim (batch_size, block_size, vocab_size) (B, T, C). Here its, [4, 8, 65]\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      # Converting 3D array to 2D array where B and T dimensions are converted to a single dimension while preserving the C dimension.\n",
        "      # This is done to conform with the cross_entropy documentation.\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets) # loss should be -ln(1/vocab_size). Check Karpathy videos for more understanding.\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"idx is (B, T) array. This function will take the input tokens `idx` and generate the output tokens based on the inputs.\n",
        "\n",
        "    Arguments:\n",
        "        idx {Array} -- An array of size (B, T)\n",
        "        max_new_tokens {Integer} -- The number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        [Array] -- The output of size (B, T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # First get the prediction using the forward function.\n",
        "      logits, loss = self(idx)\n",
        "      # Then only keep the last index of the T dimension because each of the element in T represents a token and we are only focused om the last token for each prediction.\n",
        "      logits = logits[:, -1, :] # Shape: (B, C)\n",
        "      # Apply softmax to get the probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # Shape: (B, C)\n",
        "      # Get the sample for the distribution.\n",
        "      # After the softmax, we are getting the a 2D array where each row element (which denotes the batch) has an array of length C.\n",
        "      # That array is the probability distribution of what would be the next token for each batch.\n",
        "      # Now, the multinomial function samples the probability distribution and returns the single token from the probability distribution.\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1) # Shape: (B, 1)\n",
        "      # Append the prediction to the input sequence.\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # Shape: (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "logits, loss = model(xb, yb)\n",
        "# logits, loss = model(xb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long) # Generates token 0 which corresponds to newline. This will be used to start generation\n",
        "generated_tokens = model.generate(idx, max_new_tokens = 100)[0]\n",
        "output = decode(generated_tokens.tolist())\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0A7XnQKKucP",
        "outputId": "5e6fcddf-66e5-4973-d709-3055a6621fa8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output we got was completely garbage values because the embedding table was randomly generated without any training. Lets train the model first before using it."
      ],
      "metadata": {
        "id": "3alIMLOmU07z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3) # 3r-4 is a good learning rate for Bigger models.\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K9i0a-FU_bm",
        "outputId": "7c880814-3161-48f5-a030-ccbb4a00504e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5727508068084717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long) # Generates token 0 which corresponds to newline. This will be used to start generation\n",
        "generated_tokens = model.generate(idx, max_new_tokens = 500)[0]\n",
        "output = decode(generated_tokens.tolist())\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK-HwE_XXcj4",
        "outputId": "54b64003-3d29-49c0-f826-95e5f0226d44"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht anjx?\n",
            "\n",
            "DUThinqunt.\n",
            "\n",
            "LaZAnde.\n",
            "athave l.\n",
            "KEONH:\n",
            "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
            "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
            "I hisgothers je are!-e!\n",
            "QLYotouciullle'z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The mathematical trick in Self Attention"
      ],
      "metadata": {
        "id": "6xPebZGWHpOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMMUvlS7HlEG",
        "outputId": "951c21ac-095b-477d-9466-35edc56ffc46"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1: Averaging past context with for loop, the weakest form of aggregation.\n",
        "\n",
        "# We want x[b, t] = mean_{i<=t} x[b, i]\n",
        "# A token needs the context of the prev tokens (decoder architecture) so that we can understand the context of the token with respect to the sentence and predict the next tokens.\n",
        "# Currently, we are averaging out the context of all the prev tokens (from 0 to t token) to get the new context.\n",
        "x_bag_of_words = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    x_prev = x[b, :t+1] # Dim: (t, C) because we are calculating x_prev for each batch, so there is no batch dim for this.\n",
        "    x_bag_of_words[b, t] = torch.mean(x_prev, 0) # Dim: (C) We are avg out the t dimension to get the mean of the context (C) dimension\n",
        "\n",
        "print(x[1])\n",
        "print(x_bag_of_words[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ViQOtrIKyu",
        "outputId": "71f778b9-fab0-4b8b-81ad-5ff748ae9112"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348],\n",
            "        [ 0.4478, -0.8016],\n",
            "        [ 1.5236,  2.5086]])\n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.8173,  0.4127],\n",
            "        [-0.1342,  0.4395],\n",
            "        [ 0.2711,  0.4774],\n",
            "        [ 0.2421,  0.0694],\n",
            "        [ 0.0084,  0.0020],\n",
            "        [ 0.0712, -0.1128],\n",
            "        [ 0.2527,  0.2149]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix multiply as weighted aggregation\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.ones(3, 3)\n",
        "a = torch.tril(a)\n",
        "a = a / torch.sum(a, 1, keepdim=True) # With this, the 1st row will only have the context from 1st row, the 2nd row will be the avg of 1st and 2nd row, and so on.\n",
        "# This is exactly what we did above to find the mean of the context dim but with pure maths, improving performance.\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a @ b\n",
        "\n",
        "print(f\"a = {a}\")\n",
        "print(f\"b = {b}\")\n",
        "print(f\"c = {c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0UurkYdL9nh",
        "outputId": "f2e4c606-da67-48ce-ee3f-ac7f999d7e5f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b = tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c = tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2: Self Attention using Matrix Multiplication\n",
        "weights = torch.tril(torch.ones(T, T))\n",
        "weights = weights / weights.sum(1, keepdim=True)\n",
        "\n",
        "x_bag_of_words2 = weights @ x # (B, T, T) @ (B, T, C) -> (B, T, C) [PyTorch will automatically add B dimension to the weights and do parallel multiplication]\n",
        "torch.allclose(x_bag_of_words, x_bag_of_words2) # This will come out as True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g66ViCNrQTk7",
        "outputId": "8ec0e064-e7bb-4082-c9eb-d69c75cf7d95"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3: Using softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "weights = torch.zeros((T, T))\n",
        "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "x_bow3 = weights @ x\n",
        "torch.allclose(x_bag_of_words, x_bow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCzGGCq9Vtia",
        "outputId": "93f2e53c-7b33-42f4-af3c-9a4490229ceb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}